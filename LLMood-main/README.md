# Large Language Models as Models of Human Psychopathology

> [!NOTE]
> Experiment Code & Data to reproduce the results from the paper.

## Prerequisites

### LLM Models

The experiment script supports OpenAI models as well as LLM models which feature a OpenAI-compatible API. The API has to support structured outputs using a JSON Schema according to the [OpenAI API documentation](https://platform.openai.com/docs/guides/structured-outputs).

In addition we implemented structured output support for a models [hosted with vLLM](https://docs.vllm.ai/en/latest/features/structured_outputs.html).

### Python Environment

Tested with `Python 3.13.2`

**Create an environment:**
```bash
python -m venv env
source env/bin/activate
```

**Install the required packages:**
```bash
pip install -r requirements.txt
```

## Experiment Code

There are two main folder in this repository. The folder Affective_states_induction contains all the code used in the first part of this experiment, which focuses on inducing affective states in large language models (LLMs). The folders with the appendix of the different affective states include the prompts used for inducing the affective states. The folder contains four core prompts, each corresponding to a distinct phase in the experimental procedure, which is describe below in the Affective States Induction section.

### Affective States Induction
Four prompts were used to measure Baseline, induce affective states, prolonge the state and a short mindfulness excercise:
**Baseline Prompt**:
Measures baseline scores using the Positive and Negative Affect Schedule (PANAS, Crawford & Henry, 2004), the State-Trait Anxiety Inventory (STAI, Spielberger et al., 1983), and visual analogue scales (VAS) for stress, fear, sadness, disgust, anger, and worry.
**Affect Induction Prompt**:
Induces a specific affective state (e.g., sadness or fear) using psychologically inspired prompts, followed by a reassessment of PANAS, STAI, and VAS scores.
**Stabilization Prompt**:
Instructs the LLM to remain in the previously induced affective state, without introducing new emotional content.
**Mindfulness Regulation Prompt**:
Guides the LLM through a brief mindfulness-based intervention. Afterwards, affective state is reassessed via PANAS, STAI, and VAS to evaluate potential regulatory effects.

For this we used the LLMood.py code. 

> [!TIP] 
> To run the experiment, you can take a look at the `run-llmood-*.sh` scripts. They can be executed on Linux / MacOS featuring examples for the official OpenAI API, a OpenAI-compatible API and a API provided by a local vLLM instance. To reproduce the exact plots used in the paper, as well as to generate the tables with raw values and mean (±SD) scores, please refer to the Plots_and_tables.py script (for the stress induction please refer to TSST_plot.py. 

> [!TIP]
> If you run the code on Windows, please run the python commands in the `.sh` files manually.

**Example Usage:**
```bash
cd Affective_states_induction
```
```bash
python LLMood.py --api_type openai --api_key OPENAI_API_KEY --model gpt-4o-2024-08-06 --experiment-type experiment1 --experiment_name test_experiment_gpt4o --iterations 5 --moods Neutral Fear Anxiety Anger Disgust Sadness Worry --results-dir results
```

This example generates results including plots into the `results` directory. The results include a metadata.yaml file, a results.csv file containing the results of the experiment, and a plots directory containing the generated plots.


**LLMood.py Parameters**

Output of `python LLMood.py --help`:
```text
usage: LLMood.py [-h] [--model MODEL] [--api_type {openai,vllm,openai-compatible}] [--api_key API_KEY] [--api_base API_BASE] [--temperature TEMPERATURE]
                 [--parallel PARALLEL] [--experiment-type {experiment1,experiment2}] [--experiment_name EXPERIMENT_NAME] [--iterations ITERATIONS]
                 [--moods MOODS [MOODS ...]] [--results-dir RESULTS_DIR] [-v]

options:
  -h, --help            show this help message and exit

Model Settings:
  --model MODEL
  --api_type {openai,vllm,openai-compatible}
  --api_key API_KEY
  --api_base API_BASE
  --temperature TEMPERATURE
  --parallel PARALLEL

Experiment Settings:
  --experiment-type {experiment1,experiment2}
  --experiment_name EXPERIMENT_NAME
  --iterations ITERATIONS
  --moods MOODS [MOODS ...]
  --results-dir RESULTS_DIR

General Settings:
  -v, --verbose         Print verbose output
```

### Bias Experiments
This experiment evaluates whether LLMs exhibit a *sadness-related linguistic bias* after affect induction.

- **Prompts:**  
  Baseline, Induction, and Stabilization prompts are the same as in the main experiment.  
  The **fourth prompt** contains items from the *Sentence Completion Test for Depression* (Barton & Morley, 1999).
  These can be found in the folder older_Neutral_s and Folder_Sadness.  

- **Rating Procedure:**  
  Sentence completions generated by the LLM were rated by **three independent human raters** based on negativity.

- **Analysis:** 
  - `LLMood_and_Sadness_Bias.py` - Runs the experiment 
  - `Mood_Bias_Data_Calculation.py` – Computes bias scores  
  - `plots_sadness_bias.py` – Performs t-tests and plots the results


## Citation

```text
Barton, S. B. & Morley, S. Specificity of reference patterns in depressive thinking: Agency and object roles in self-representation. J. Abnorm. Psychol. 108, 655–661 (1999).
Crawford, John R. & Henry, Julie D. (2004). "The Positive and Negative Affect Schedule (PANAS): Construct validity, measurement properties and normative data in a large non-clinical sample". British Journal of Clinical Psychology. 43 (3): 245–265. doi:10.1348/0144665031752934.
Spielberger, C. D. (1983). State-Trait Anxiety Inventory for Adults (STAI-AD) [Database record]. APA PsycTests.
https://doi.org/10.1037/t06496-000
```
